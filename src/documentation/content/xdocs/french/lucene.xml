<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V1.2//EN" "http://apache.org/forrest/dtd/document-v12.dtd">
<document xml:lang="fr"> 
  <header> 
    <title>Utilisation dans Lucene</title> 
  </header> 
  <body>	
	<section>
		<title>L'analyseur arabe pour Lucene</title>
		<p>Exécutons le code suivant :</p>
		<source>
java -cp build/ArabicAnalyzer.jar;lib/commons-collections.jar;lib/lucene-1.4.3.jar ¶
gpl.pierrick.brihaye.aramorph.test.TestArabicAnalyzer ¶
src/java/gpl/pierrick/brihaye/aramorph/test/testdocs/ktAb.txt CP1256 results.txt UTF-8
		</source>		
		<warning>Naturellement, l'encodage du fichier d'entrée doit être adapté à votre plate-forme.</warning>
		<p>Examinons le fichier de sortie <code>results.txt</code> :</p>
		<source>
كِتاب	NOUN	[0-4]	1
كُتّاب	NOUN	[0-4]	0	
		</source>		
		<p>Le principe est donc le suivant : chaque <strong>radical</strong> pertinent retourne un 
			<fork href="http://jakarta.apache.org/lucene/docs/api/org/apache/lucene/analysis/Token.html"><em>token</em></fork> 
			au sens où l'entend Lucene. Ici apparaissent le texte du <em>token</em> 
			(<fork href="http://jakarta.apache.org/lucene/docs/api/org/apache/lucene/analysis/Token.html#termText()"><code>termText</code></fork>),
			sa catégorie grammaticale, 
			sa position dans le flux d'entrée (<fork href="http://jakarta.apache.org/lucene/docs/api/org/apache/lucene/analysis/Token.html#startOffset()"><code>startOffset</code></fork> et 
			<fork href="http://jakarta.apache.org/lucene/docs/api/org/apache/lucene/analysis/Token.html#endOffset()"><code>endOffset</code></fork>)			
			et sa position relative par rapport au <em>token</em> précédent (<fork href="http://jakarta.apache.org/lucene/docs/api/org/apache/lucene/analysis/Token.html#getPositionIncrement()"><code>positionIncrement</code></fork>).</p>
		<p>Il faut en effet attirer l'attention sur le fait qu'un même mot arabe, parce qu'il est généralement réduit à son squelette consonnantique, donne souvent lieu à <strong>plusieurs</strong> <em>solutions</em>.</p>
		<fixme author="PB">Quand bien même le texte serait vocalisé, les voyelles ne sont malheureusement pas prises en compte pour lever les ambiguités d'analyse. La résolution de ce problème est <link href="site:todo-fr">à l'étude</link>.</fixme>
		<note>Dans cet exemple, le texte des <em>tokens</em> est en arabe, mais pour des raisons de performance, il vaut sans doute mieux retourner des tokens translitérés selon le système de Buckwalter en ne spécifiant pas d'encodage de sortie pour le fichier de résultats.</note>
		<warning>Dans cet exemple, et bien que <em>كتاب</em> ait 3 solutions, on ne retourne que 2 <em>tokens</em>. En effet, <em>كُتّاب</em> est présent par 2 fois, la première en tant que pluriel de <em>كاتب</em>, la seconde en tant que nom singulier valant pour <em>école coranique</em>.</warning>
		<fixme author="PB">Que faire si un même texte de <em>token</em> renvoie des catégories grammaticales différentes ? Etant donné que les index <em>Lucene</em> perdent l'information de type, ce problème n'en est peut-être pas un.</fixme>
		<p>Essayons un autre exemple avec un fichier <code>tktbn.txt</code> contenant le mot <em>تكتبن</em> :</p>
		<source>
java -cp build/ArabicAnalyzer.jar;lib/commons-collections.jar;lib/lucene-1.4.3.jar ¶
gpl.pierrick.brihaye.aramorph.AraMorph ¶
src/java/gpl/pierrick/brihaye/aramorph/test/testdocs/tktbn.txt CP1256 results.txt UTF-8</source>
		<p>Examinons le fichier de sortie <code>results.txt</code> :</p>		
		<source>
Processing token : 	?????

SOLUTION #3
Lemma  : 	>akotab
Vocalized as : 	tukotibna
Morphology : 
	prefix : IVPref-Antn-tu
	stem : IV_yu
	suffix : IVSuff-n
Grammatical category : 
	prefix : tu	IV2FP
	stem : kotib	VERB_IMPERFECT
	suffix : na	IVSUFF_SUBJ:FP
Glossed as : 
	prefix : you [fem.pl.]
	stem : dictate/make write
	suffix : [fem.pl.]


SOLUTION #4
Lemma  : 	>akotab
Vocalized as : 	tukotabna
Morphology : 
	prefix : IVPref-Antn-tu
	stem : IV_Pass_yu
	suffix : IVSuff-n
Grammatical category : 
	prefix : tu	IV2FP
	stem : kotab	VERB_IMPERFECT
	suffix : na	IVSUFF_SUBJ:FP
Glossed as : 
	prefix : you [fem.pl.]
	stem : be dictated
	suffix : [fem.pl.]


SOLUTION #2
Lemma  : 	katab
Vocalized as : 	tukotabna
Morphology : 
	prefix : IVPref-Antn-tu
	stem : IV_Pass_yu
	suffix : IVSuff-n
Grammatical category : 
	prefix : tu	IV2FP
	stem : kotab	VERB_IMPERFECT
	suffix : na	IVSUFF_SUBJ:FP
Glossed as : 
	prefix : you [fem.pl.]
	stem : be written/be fated/be destined
	suffix : [fem.pl.]


SOLUTION #1
Lemma  : 	katab
Vocalized as : 	takotubna
Morphology : 
	prefix : IVPref-Antn-ta
	stem : IV
	suffix : IVSuff-n
Grammatical category : 
	prefix : ta	IV2FP
	stem : kotub	VERB_IMPERFECT
	suffix : na	IVSUFF_SUBJ:FP
Glossed as : 
	prefix : you [fem.pl.]
	stem : write
	suffix : [fem.pl.]	
		</source>
		<p>Ici, le découpage en préfixe, radical et suffixe devient très apparent.</p>
		<warning label="Rappel">La catégorie grammaticale du préfixe, du radical et du suffixe ont été ventilées par <em>AraMorph</em>. En d'autres termes, le découpage grammatical est peut être différent du découpage morphologique.</warning>
		<p>Pour bien comprendre que l'analyse ne retourne <strong>que</strong> le radical :</p>
		<source>
java -cp build/ArabicAnalyzer.jar;lib/commons-collections.jar;lib/lucene-1.4.3.jar ¶
gpl.pierrick.brihaye.aramorph.test.TestArabicAnalyzer ¶
src/java/gpl/pierrick/brihaye/aramorph/test/testdocs/tktbn.txt results.txt
		</source>
		<p>Examinons le fichier de sortie <code>results.txt</code> :</p>		
		<source>
kotub	VERB_IMPERFECT	[0-5]	1
kotab	VERB_IMPERFECT	[0-5]	0
kotib	VERB_IMPERFECT	[0-5]	0
		</source>
		<p>On constate que l'on obtient bien les radicaux des différentes formes que peut recouvrir la racine <em>كتب</em> pour un verbe à l'inaccompli. Ainsi, selon ce système, l'analyse d'un verbe à la deuxième personne du féminin pluriel de l'inaccompli est la <strong>même</strong> que celle de toute autre personne au même temps.</p>
		<note>Cette implémentation peut tout à fait être rediscutée !</note>
	</section>
	<section>
		<title>L'analyseur anglais pour Lucene</title>
		<p>Exécutons le code suivant :</p>
		<source>
java -cp build/ArabicAnalyzer.jar;lib/commons-collections.jar;lib/lucene-1.4.3.jar ¶
gpl.pierrick.brihaye.aramorph.test.TestArabicGlosser ¶
src/java/gpl/pierrick/brihaye/aramorph/test/testdocs/ktAb.txt CP1256 results.txt UTF-8
		</source>				
		<warning>Naturellement, l'encodage du fichier d'entrée doit être adapté à votre plate-forme.</warning>
		<p>Examinons le fichier de sortie <code>results.txt</code> :</p>
		<source>
kuttab	NOUN	[0-4]	1
village	NOUN	[0-4]	0
school	NOUN	[0-4]	0
quran	NOUN	[0-4]	0
school	NOUN	[0-4]	0
authors	NOUN	[0-4]	0
writers	NOUN	[0-4]	0
book	NOUN	[0-4]	0
		</source>		
		<p>Ici, le principe est strictement le même si ce n'est que les glosses sont elles-mêmes tokenisées par le <fork href="http://www.nongnu.org/aramorph/javadoc/gpl/pierrick/brihaye/aramorph/lucene/WhitespaceFilter.html"><code>WhitespaceFilter</code></fork> avant d'intégrer la chaîne de traitement standard de Lucene (<fork href="http://jakarta.apache.org/lucene/docs/api/org/apache/lucene/analysis/standard/StandardFilter.html"><code>StandardFilter</code></fork>, <fork href="http://jakarta.apache.org/lucene/docs/api/org/apache/lucene/analysis/LowerCaseFilter.html"><code>LowerCaseFilter</code></fork> et <fork href="http://jakarta.apache.org/lucene/docs/api/org/apache/lucene/analysis/StopFilter.html"><code>StopFilter</code></fork>).</p>
		<note>Le type de <em>token</em> est le type du radical <strong>arabe</strong> analysé ; il va de soi que le type de mot <strong>anglais</strong> peut être différent.</note>
		<note>Cette implémentation peut tout à fait être rediscutée !</note>
	</section>
	<section>
	<title>Quelles sont les solutions retenues par les analyseurs ?</title>
	<p>Comme on peut le voir, les analyseurs renvoient des <em>tokens</em> dont le type correspond à la catégorie grammaticale du <strong>radical</strong>.<br/>
	Cependant, un analyseur considèrera certaines catégories grammaticales comme attribuables à des <strong>mots vides</strong> et ne renverra donc aucun <em>token</em> pour ce radical. La liste des catégories grammaticales non signifiantes est décrite ci-après :</p>
	<ul>		
	<li><code>DEM_PRON_F</code></li>
	<li><code>DEM_PRON_FS</code></li>
	<li><code>DEM_PRON_FD</code></li>
	<li><code>DEM_PRON_MD</code></li>
	<li><code>DEM_PRON_MP</code></li>
	<li><code>DEM_PRON_MS</code></li>
	<li><code>DET</code></li>		
	<li><code>INTERROG</code></li>	
	<li><code>NO_STEM</code></li>			
	<li><code>NUMERIC_COMMA</code></li>															
	<li><code>PART</code></li>	
	<li><code>PRON_1P</code></li>
	<li><code>PRON_1S</code></li>
	<li><code>PRON_2D</code></li>
	<li><code>PRON_2FP</code></li>
	<li><code>PRON_2FS</code></li>
	<li><code>PRON_2MP</code></li>
	<li><code>PRON_2MS</code></li>
	<li><code>PRON_3D</code></li>
	<li><code>PRON_3FP</code></li>
	<li><code>PRON_3FS</code></li>
	<li><code>PRON_3MP</code></li>														
	<li><code>PRON_3MS</code></li>	
	<li><code>REL_PRON</code></li>														
	</ul>
	<p>A contrario, voici la liste des catégories grammaticales <strong>signifiantes</strong> : </p>	
	<ul>
		<li><code>ABBREV</code></li>
		<li><code>ADJ</code></li>		
		<li><code>ADV</code></li>					
		<li><code>NOUN</code></li>		
		<li><code>NOUN_PROP</code></li>		
		<li><code>VERB_IMPERATIVE</code></li>		
		<li><code>VERB_IMPERFECT</code></li>		
		<li><code>VERB_PERFECT</code></li>			
		<li><code>NO_RESULT</code>
		<warning>Mot conservé dans la mesure où l'expérience semble montrer qu'il y a de fortes chances qu'il soit un mot <strong>étranger</strong> absent du dictionnaire. Il est naturellement tout à fait possible d'écrire un <fork href="http://jakarta.apache.org/lucene/docs/api/org/apache/lucene/analysis/TokenFilter.html">filtre <em>Lucene</em> spécifique</fork> pour affiner l'analyse de ce type de <em>token</em>.</warning></li>
	</ul>
	<note label="Rappel">Les explications sur les catégories grammaticales sont disponibles dans <link href="site:grammatical-categories-fr">cette rubrique</link>.</note>		
	<note>Cette implémentation peut tout à fait être rediscutée !</note>
	</section>
  </body>  
</document>
